ggplot(data = ama_train) +
geom_count(mapping = aes(x = ama_train$ROLE_ROLLUP_1, y = ama_train$ROLE_TITLE))
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
ama_train <- vroom("C:/School/Stat348/AmazonEmployeeAccess/train.csv")
ama_test <- vroom("C:/School/Stat348/AmazonEmployeeAccess/test.csv")
ggplot(ama_train) +
geom_bar(aes(x=ama_train$ACTION))
ggplot(ama_train) +
geom_box(aes(x=ama_train$RESOURCE, y = ACTION))
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
ama_train <- vroom("C:/School/Stat348/AmazonEmployeeAccess/train.csv")
ama_test <- vroom("C:/School/Stat348/AmazonEmployeeAccess/test.csv")
# ggplot(ama_train) +
#   geom_bar(aes(x=ama_train$ACTION))
#
# ggplot(ama_train) +
#   geom_box(aes(x=ama_train$RESOURCE, y = ACTION))
#
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
model <- glm(ACTION ~ ., data = ama_train, family = "binomial")
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data=ama_train)
# Fit logistic regression model
my_mod <- glm(ACTION ~ ., data = ama_train, family = "binomial")
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data=ama_train)
# Fit logistic regression model
my_mod <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data=ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) %>%
mutate( Id = row_number()) %>%
rename(Action=.pred) %>%
select(Id, Action)
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod) %>%
fit(data=ama_train)
ama_train <- vroom("C:/School/Stat348/AmazonEmployeeAccess/train.csv")
ama_test <- vroom("C:/School/Stat348/AmazonEmployeeAccess/test.csv")
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate(ACTION = as.factor(ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) #%>%
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate(ACTION = as.factor(ama_train$ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) #%>%
ama_train <- vroom("C:/School/Stat348/AmazonEmployeeAccess/train.csv")
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=2) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c(2)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c(1,2)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c(1:2)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c('1', '2')) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c('1', '0')) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_num2factor(ACTION, levels=c('0', '1')) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate(Action = as.factor(ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
# Fit logistic regression model
my_model <- logistic_reg(Action ~ .) %>%
set_engine('glm')
# Fit logistic regression model
my_model <- logistic_reg(Action ~ .) %>%
set_engine('glm')
?logicstic_reg
?logicstic_reg()
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate(ACTION = as.factor(ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) #%>%
ama_predictions <- predict(ama_workflow, new_data=ama_test) %>%
mutate(Id = row_number()) %>%
rename(ACTION =.pred) %>%
select(Id, Action)
ama_train <- vroom("C:/School/Stat348/AmazonEmployeeAccess/train.csv")
ama_test <- vroom("C:/School/Stat348/AmazonEmployeeAccess/test.csv")
# ggplot(ama_train) +
#   geom_bar(aes(x=ama_train$ACTION))
#
# ggplot(ama_train) +
#   geom_box(aes(x=ama_train$RESOURCE, y = ACTION))
my_recipe <- recipe(ACTION ~ ., data=ama_train) %>%
step_mutate(ACTION = as.factor(ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data=ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) %>%
mutate(Id = row_number()) %>%
rename(ACTION =.pred) %>%
select(Id, Action)
my_recipe <- recipe(ACTION ~ ., data = ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data = ama_train)
my_recipe <- recipe(ACTION ~ ., data = ama_train) %>%
step_mutate(ACTION = as.factor(ACTION)) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data = ama_train)
ama_predictions <- predict(ama_workflow, new_data=ama_test) %>%
mutate(Id = row_number()) %>%
rename(ACTION =.pred) %>%
select(Id, Action)
my_recipe <- recipe(ACTION ~ ., data = ama_train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = ama_train)
# Fit logistic regression model
my_model <- logistic_reg() %>%
set_engine('glm')
ama_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model) %>%
fit(data = ama_train)
library(MASS)
install.packages('MASS')
library(MASS)
install.packages("MASS")
install.packages("MASS")
read(crabs)
read.csv(crabs)
read.csv(MASS::crabs)
data('crabs')
library(MASS)
data('crabs')
install.packages('Rserve')
library(Rserve)
Rserve(args="--no-save")
?predict()
setwd("C:/School/Stat348")
setwd("C:/School/Stat348/DontGetKicked")
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(bonsai)
library(lightgbm)
train <- vroom("./training.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(IsBadBuy = as.factor(IsBadBuy))
my_recipe <- recipe(IsBadBuy ~ ., data=train) %>%
update_role(RefId, new_role = "id variable") %>%
step_mutate_at(Color, fn = factor) %>%# turn color into factors
step_lencode_glm(Color, outcome=vars(IsBadBuy))
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
boost_model <- boost_tree(tree_depth=tune(),
trees=tune(),
learn_rate=tune()) %>%
set_engine("lightgbm") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
boost_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(boost_model)
boost_tuning_grid <- grid_regular(tree_depth(),
trees(),
learn_rate(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train, v = 5, repeats=1)
boost_CV_results <- boost_workflow %>%
tune_grid(resamples=folds,
grid=boost_tuning_grid,
metrics=metric_set(accuracy)) #Or leave metrics NULL
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(bonsai)
library(lightgbm)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(13) # num_cores to use
registerDoParallel(cl)
train <- vroom("./training.csv")
test <- vroom("./test.csv")
train <- train %>%
mutate(IsBadBuy = as.factor(IsBadBuy))
my_recipe <- recipe(IsBadBuy ~ ., data=train) %>%
update_role(RefId, new_role = "id variable") %>%
step_mutate_at(Color, fn = factor) %>%# turn color into factors
step_lencode_glm(Color, outcome=vars(IsBadBuy))
#step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
#step_dummy(color) # dummy variable encoding
#step_lencode_mixed(color, outcome = vars(type)) #%>% #target encoding
#step_smote(all_outcomes(), k=2)
# also step_lencode_glm() and step_lencode_bayes()
# NOTE: some of these step functions are not appropriate to use together
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data = train)
boost_model <- boost_tree(tree_depth=tune(),
trees=tune(),
learn_rate=tune()) %>%
set_engine("lightgbm") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
# bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
#   set_engine("dbarts") %>% # might need to install
#   set_mode("classification")
#
# bart_workflow <- workflow() %>%
#   add_recipe(my_recipe) %>%
#   add_model(bart_model)
boost_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(boost_model)
## Grid of values to tune over
# bart_tuning_grid <- grid_regular(trees(),
#                                  levels = 5) ## L^2 total tuning possibilities
boost_tuning_grid <- grid_regular(tree_depth(),
trees(),
learn_rate(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train, v = 5, repeats=1)
## Run the CV
# bart_CV_results <- bart_workflow %>%
#   tune_grid(resamples=folds,
#             grid=bart_tuning_grid,
#             metrics=metric_set(accuracy))
boost_CV_results <- boost_workflow %>%
tune_grid(resamples=folds,
grid=boost_tuning_grid,
metrics=metric_set(accuracy)) #Or leave metrics NULL
stopCluster(cl)
library(vroom)
library(tidyverse)
library(tidymodels)
library(embed)
library(doParallel)
library(themis)
library(bonsai)
library(lightgbm)
#parallel::detectCores() #How many cores do I have?
cl <- makePSOCKcluster(13) # num_cores to use
registerDoParallel(cl)
train <- vroom("training.csv", na = c("", "NA", "NULL", "NOT AVAIL"))
test <- vroom("test.csv", na = c("", "NA", "NULL", "NOT AVAIL"))
my_recipe <- recipe(IsBadBuy ~., data = train) %>%
update_role(RefId, new_role = 'ID') %>%
update_role_requirements("ID", bake = FALSE) %>%
step_mutate(IsBadBuy = factor(IsBadBuy), skip = TRUE) %>%
step_mutate(IsOnlineSale = factor(IsOnlineSale)) %>%
step_mutate_at(all_nominal_predictors(), fn = factor) %>%
#step_mutate_at(contains("MMR"), fn = numeric) %>%
step_rm(contains('MMR')) %>%
step_rm(BYRNO, WheelTypeID, VehYear, VNST, VNZIP1, PurchDate, # these variables don't seem very informative, or are repetitive
AUCGUART, PRIMEUNIT, # these variables have a lot of missing values
Model, SubModel, Trim) %>% # these variables have a lot of levels - could also try step_other()
step_corr(all_numeric_predictors(), threshold = .7) %>%
step_other(all_nominal_predictors(), threshold = .0001) %>%
step_novel(all_nominal_predictors()) %>%
step_unknown(all_nominal_predictors()) %>%
#step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(IsBadBuy))%>% # target encoding
step_impute_median(all_numeric_predictors())
#
# train <- vroom("./training.csv")
# test <- vroom("./test.csv")
#
# train <- train %>%
#   mutate(IsBadBuy = as.factor(IsBadBuy))
# my_recipe <- recipe(IsBadBuy ~ ., data=train) %>%
#   update_role(RefId, new_role = "id variable") %>%
#   step_mutate_at(Color, fn = factor) %>%# turn color into factors
#   step_lencode_glm(Color, outcome=vars(IsBadBuy))
#step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
#step_dummy(color) # dummy variable encoding
# apply the recipe to your data
prep <- prep(my_recipe)
baked <- bake(prep, new_data=test)
baked <- bake(prep, new_data = train)
boost_model <- boost_tree(tree_depth=tune(),
trees=tune(),
learn_rate=tune()) %>%
set_engine("lightgbm") %>% #or "xgboost" but lightgbm is faster
set_mode("classification")
# bart_model <- bart(trees=tune()) %>% # BART figures out depth and learn_rate
#   set_engine("dbarts") %>% # might need to install
#   set_mode("classification")
#
# bart_workflow <- workflow() %>%
#   add_recipe(my_recipe) %>%
#   add_model(bart_model)
boost_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(boost_model)
## Grid of values to tune over
# bart_tuning_grid <- grid_regular(trees(),
#                                  levels = 5) ## L^2 total tuning possibilities
boost_tuning_grid <- grid_regular(tree_depth(),
trees(),
learn_rate(),
levels = 5) ## L^2 total tuning possibilities
## Split data for CV
folds <- vfold_cv(train, v = 5, repeats=1)
## Run the CV
# bart_CV_results <- bart_workflow %>%
#   tune_grid(resamples=folds,
#             grid=bart_tuning_grid,
#             metrics=metric_set(accuracy))
boost_CV_results <- boost_workflow %>%
tune_grid(resamples=folds,
grid=boost_tuning_grid,
metrics=metric_set(gain_capture)) #Or leave metrics NULL
## Find Best Tuning Parameters
# bart_bestTune <- bart_CV_results %>%
#   select_best()
boost_bestTune <- boost_CV_results %>%
select_best()
## Finalize the Workflow & fit it
# bart_final_wf <-
#   bart_workflow %>%
#   finalize_workflow(bart_bestTune) %>%
#   fit(data=train)
boost_final_wf <-
boost_workflow %>%
finalize_workflow(boost_bestTune) %>%
fit(data=train)
## Predict
# car_predictions <- predict(bart_final_wf, new_data=test, type='class') %>%
#   rename(type=.pred_class) %>%
#   mutate(RefId = test$RefId) %>%
#   select(RefId, IsBadBuy)
#
# vroom_write(x=car_predictions, file="./bart.csv", delim=",")
#
car_predictions <- predict(boost_final_wf, new_data=test, type='prob') %>%
rename(IsBadBuy=.pred_1) %>%
mutate(RefId = test$RefId) %>%
select(RefId, IsBadBuy)
vroom_write(x=car_predictions, file="./boost.csv", delim=",")
stopCluster(cl)
issingular
isSingular
